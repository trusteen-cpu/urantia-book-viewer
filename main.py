import streamlit as st
import pandas as pd
import re
import os

# ------------------------------------------------------------
# í˜ì´ì§€ ì„¤ì •
# ------------------------------------------------------------
st.set_page_config(page_title="Urantia Book Viewer", layout="wide")

# ------------------------------------------------------------
# íŒŒì¼ ê²½ë¡œ
# ------------------------------------------------------------
KO_PATH = os.path.join("data", "urantia_ko.txt")
EN_PATH = os.path.join("data", "urantia_en.txt")
GLOSSARY_PATH = os.path.join("data", "glossary.xlsx")

# ------------------------------------------------------------
# ì•ˆì „í•œ íŒŒì¼ ì½ê¸°
# ------------------------------------------------------------
def safe_read_lines(path):
    encodings = ["utf-8", "utf-8-sig", "cp949", "euc-kr", "utf-16", "latin-1"]
    for enc in encodings:
        try:
            with open(path, "r", encoding=enc) as f:
                return f.readlines()
        except Exception:
            continue
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        return f.readlines()

def clean_text(t: str) -> str:
    return t.replace("\ufeff", "").replace("ï¿½", "").strip()

@st.cache_data
def load_texts():
    def parse_file(path):
        data = {}
        lines = safe_read_lines(path)
        for line in lines:
            line = line.strip()
            m = re.match(r"^(\d+:\d+\.\d+)\s+(.*)$", line)
            if m:
                data[m.group(1)] = clean_text(m.group(2))
        return data

    ko = parse_file(KO_PATH)
    en = parse_file(EN_PATH)
    return ko, en

@st.cache_data
def load_glossary():
    df = pd.read_excel(GLOSSARY_PATH)
    df.columns = df.columns.str.lower()
    return df

ko_texts, en_texts = load_texts()
glossary = load_glossary()

# ------------------------------------------------------------
# ì°¸ì¡° ê²€ìƒ‰ ë¡œì§
# ------------------------------------------------------------
def get_pairs_by_ref(ref: str):
    pairs = []
    if re.match(r"^\d+:\d+\.\d+$", ref):  # ì ˆ
        if ref in ko_texts:
            pairs.append((ref, ko_texts[ref], en_texts.get(ref, "")))
        return pairs
    if re.match(r"^\d+:\d+$", ref):  # ì¥
        prefix = ref + "."
        for k in ko_texts:
            if k.startswith(prefix):
                pairs.append((k, ko_texts[k], en_texts.get(k, "")))
        return pairs
    if re.match(r"^\d+$", ref):  # í¸
        prefix = ref + ":"
        for k in ko_texts:
            if k.startswith(prefix):
                pairs.append((k, ko_texts[k], en_texts.get(k, "")))
        return pairs
    return pairs

# ------------------------------------------------------------
# ìŠ¤íƒ€ì¼
# ------------------------------------------------------------
st.markdown("""
<style>
.block-container { max-width: 95vw !important; }
.viewer-wrapper { width: 100%; margin: 0 auto; }
.verse-row { display: flex; gap: 20px; margin-bottom: 14px; align-items: flex-start; }
.verse-col {
  flex: 1;
  padding: 18px;
  background: #f9f9f9;
  border-radius: 12px;
  box-shadow: 0 0 8px rgba(0,0,0,0.05);
  line-height: 1.8;
  font-size: 17px;
  min-height: 100%;
}
.section-title { font-weight: bold; margin-bottom: 6px; }
.glossary-box {
  background: #f0f0ff;
  border-radius: 8px;
  padding: 10px 14px;
  margin-top: 18px;
}
</style>
""", unsafe_allow_html=True)

# ------------------------------------------------------------
# UI
# ------------------------------------------------------------
st.title("ğŸ“˜ Urantia Book Viewer")
st.caption("Parallel Korean-English text with glossary search and wide layout")

# ğŸ”¹ ì°¸ì¡° ê²€ìƒ‰
ref = st.text_input("ì°¸ì¡°ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 196, 196:2, 196:2.3)", "", key="ref_input").strip()

if ref:
    pairs = get_pairs_by_ref(ref)
    if not pairs:
        st.warning("ì¼ì¹˜í•˜ëŠ” ë³¸ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. ì˜ˆ: 196, 196:2, 196:2.3 í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ ë³´ì„¸ìš”.")
    else:
        if re.match(r"^\d+:\d+\.\d+$", ref):
            st.markdown(f"### {ref}")
        elif re.match(r"^\d+:\d+$", ref):
            st.markdown(f"### ğŸ“– Section {ref}")
        else:
            st.markdown(f"### ğŸ“œ Paper {ref}")

        html = []
        for k, ko, en in pairs:
            html.append(f"""
            <div class='verse-row'>
                <div class='verse-col'><b>{k}</b><br>{ko}</div>
                <div class='verse-col'><b>{k}</b><br>{en}</div>
            </div>
            """)
        st.markdown("<div class='viewer-wrapper'>" + "\n".join(html) + "</div>", unsafe_allow_html=True)
else:
    st.info("ì˜ˆ: 196 (í¸), 196:2 (ì¥), 196:2.3 (ì ˆ) í˜•íƒœë¡œ ê²€ìƒ‰í•´ ë³´ì„¸ìš”.")

# ------------------------------------------------------------
# ğŸ” ìš©ì–´ ê²€ìƒ‰
# ------------------------------------------------------------
st.markdown("---")
st.subheader("ğŸ” ìš©ì–´ ê²€ìƒ‰ (Glossary Search)")
term = st.text_input("ì°¾ê³  ì‹¶ì€ ìš©ì–´ (ì˜ì–´ ë˜ëŠ” í•œêµ­ì–´):", "", key="glossary_input")

if term:
    results = glossary[
        glossary["term-ko"].str.contains(term, case=False, na=False)
        | glossary["term-en"].str.contains(term, case=False, na=False)
    ]
    if not results.empty:
        st.markdown("#### ğŸ“– ê²€ìƒ‰ ê²°ê³¼")
        for _, row in results.iterrows():
            st.markdown(f"""
            <div class='glossary-box'>
            <b>{row['term-ko']}</b> / *{row['term-en']}*  
            â€” {row['description']}
            </div>
            """, unsafe_allow_html=True)
    else:
        st.info("ì¼ì¹˜í•˜ëŠ” ìš©ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. ì˜ˆ: â€˜ì‹ ë¹„ ëª¨ë‹ˆí„°â€™, â€˜Thought Adjusterâ€™, â€˜Nebadonâ€™ ë“±ì„ ì…ë ¥í•´ ë³´ì„¸ìš”.")
else:
    st.caption("ì˜ˆ: â€˜ì‹ ë¹„ ëª¨ë‹ˆí„°â€™, â€˜Thought Adjusterâ€™, â€˜Nebadonâ€™ ë“±ì„ ì…ë ¥í•´ ë³´ì„¸ìš”.")



