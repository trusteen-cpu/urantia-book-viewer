import streamlit as st
import pandas as pd
import re
import os
import chardet

KO_PATH = os.path.join("data", "urantia_ko.txt")
EN_PATH = os.path.join("data", "urantia_en.txt")
GLOSSARY_PATH = os.path.join("data", "glossary.xlsx")

# --- ë°ì´í„° ë¡œë“œ ---
@st.cache_data
def load_texts():
    def parse_file(path):
        data = {}
        try:
            with open(path, "rb") as fb:
                raw = fb.read()
                enc = chardet.detect(raw)["encoding"]
                text = raw.decode(enc, errors="replace").splitlines()
            for line in text:
                line = line.strip()
                match = re.match(r"(\d+:\d+\.\d+)\s+(.*)", line)
                if match:
                    key = match.group(1).strip()
                    data[key] = match.group(2).strip()
        except Exception as e:
            st.error(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {path} â€” {e}")
        return data or {}

    return parse_file(KO_PATH), parse_file(EN_PATH)

@st.cache_data
def load_glossary():
    try:
        df = pd.read_excel(GLOSSARY_PATH)
        df.columns = df.columns.str.lower()
        return df
    except Exception as e:
        st.warning(f"âš ï¸ ìš©ì–´ì§‘ ë¶ˆëŸ¬ì˜¤ê¸° ì˜¤ë¥˜: {e}")
        return pd.DataFrame(columns=["term-ko", "term-en", "description"])

ko_texts, en_texts = load_texts()
glossary = load_glossary()

# --- UI ---
st.title("ğŸ“˜ Urantia Book Viewer")
st.caption("Parallel Korean-English Viewer with Glossary")

input_ref = st.text_input("Enter reference (e.g. 111:7.5)", "")

# --- ê²€ìƒ‰ ë¡œì§ ---
if input_ref:
    input_ref = input_ref.strip()

    def clean_text(t):
        return t.replace("\ufeff", "").replace("ï¿½", "").strip()

    if input_ref in ko_texts:
        col1, col2 = st.columns(2)

        with col1:
            st.markdown(f"### ğŸ‡°ğŸ‡· {input_ref}")
            st.markdown(
                f"""
                <div style="background-color:#f8f8f8; padding:10px; border-radius:10px; line-height:1.8;">
                    {clean_text(ko_texts[input_ref])}
                </div>
                """,
                unsafe_allow_html=True,
            )

        with col2:
            st.markdown(f"### ğŸ‡ºğŸ‡¸ {input_ref}")
            st.markdown(
                f"""
                <div style="background-color:#f8f8f8; padding:10px; border-radius:10px; line-height:1.8;">
                    {clean_text(en_texts.get(input_ref, "âŒ No English text found."))}
                </div>
                """,
                unsafe_allow_html=True,
            )
    else:
        st.warning("No matching text found. Try nearby references or check your input.")

# --- ìš©ì–´ì§‘ ê²€ìƒ‰ ---
search_term = st.text_input("ğŸ” Search glossary term (English or Korean):", "")
if search_term:
    results = glossary[
        glossary["term-ko"].str.contains(search_term, case=False, na=False)
        | glossary["term-en"].str.contains(search_term, case=False, na=False)
    ]
    if not results.empty:
        st.write("### ğŸ“– Glossary Results")
        for _, row in results.iterrows():
            st.markdown(f"**{row['term-ko']}** / *{row['term-en']}* â€” {row['description']}")
    else:
        st.info("No matching term found in glossary.")
