import streamlit as st
import pandas as pd
import re
import os
import chardet

# --- íŒŒì¼ ê²½ë¡œ ì„¤ì • ---
KO_PATH = os.path.join("data", "urantia_ko.txt")
EN_PATH = os.path.join("data", "urantia_en.txt")
GLOSSARY_PATH = os.path.join("data", "glossary.xlsx")

# --- í…ìŠ¤íŠ¸ íŒŒì¼ ì¸ì½”ë”© ìë™ ê°ì§€ ---
def detect_encoding(path):
    with open(path, "rb") as f:
        result = chardet.detect(f.read())
    return result["encoding"] or "utf-8"

# --- í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ---
@st.cache_data
def load_texts():
    def parse_file(path):
        data = {}
        encoding = detect_encoding(path)
        with open(path, "r", encoding=encoding, errors="ignore") as f:
            for line in f:
                line = line.strip()
               match = re.match(r"(\d+(?::\d+(?:\.\d+)?)?)\s+(.*)", line)
                if match:
                    key = match.group(1).strip()
                    text = match.group(2).strip()
                    data[key] = text
        return data
    return parse_file(KO_PATH), parse_file(EN_PATH)

# --- ìš©ì–´ì§‘ ë¡œë“œ ---
@st.cache_data
def load_glossary():
    df = pd.read_excel(GLOSSARY_PATH)
    df.columns = df.columns.str.lower()
    expected_cols = {"term-ko", "term-en", "description"}
    missing = expected_cols - set(df.columns)
    if missing:
        st.warning(f"âš ï¸ ìš©ì–´ì§‘ì— ë‹¤ìŒ ì—´ì´ ì—†ìŠµë‹ˆë‹¤: {missing}")
    return df

ko_texts, en_texts = load_texts()
glossary = load_glossary()

# --- UI ì œëª© ---
st.title("ğŸ“˜ Urantia Book Viewer")
st.caption("Parallel Englishâ€“Korean viewer with glossary reference")

# --- ì…ë ¥ì°½ ---
input_ref = st.text_input("Enter reference (ì˜ˆ: 111:7 ë˜ëŠ” 111:7.5)", "")

# --- ê²€ìƒ‰ ë¡œì§ ---
def get_section_texts(ref):
    """111:7 í˜•ì‹ ì…ë ¥ ì‹œ í•´ë‹¹ ì¥ ì „ì²´ ë°˜í™˜"""
    if "." not in ref:  # ì¥ ë‹¨ìœ„ ê²€ìƒ‰
        prefix = ref + "."
        ko_results = {k: v for k, v in ko_texts.items() if k.startswith(prefix)}
        en_results = {k: v for k, v in en_texts.items() if k.startswith(prefix)}
    else:  # ì ˆ ë‹¨ìœ„ ê²€ìƒ‰
        ko_results = {ref: ko_texts.get(ref, "âŒ í•œê¸€ ë³¸ë¬¸ ì—†ìŒ")}
        en_results = {ref: en_texts.get(ref, "âŒ ì˜ì–´ ë³¸ë¬¸ ì—†ìŒ")}
    return ko_results, en_results

if input_ref:
    input_ref = input_ref.strip()
    ko_results, en_results = get_section_texts(input_ref)

    if ko_results:
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ‡°ğŸ‡· Korean Translation")
            for k, v in ko_results.items():
                st.markdown(f"**{k}**  \n{v}")
        with col2:
            st.subheader("ğŸ‡ºğŸ‡¸ English Original")
            for k, v in en_results.items():
                st.markdown(f"**{k}**  \n{v}")
    else:
        st.warning("âŒ No matching text found. Check your reference (ì˜ˆ: 111:7.5).")

# --- ìš©ì–´ì§‘ ê²€ìƒ‰ ---
search_term = st.text_input("ğŸ” Search glossary term (English or Korean):", "")
if search_term:
    results = glossary[
        glossary["term-ko"].str.contains(search_term, case=False, na=False)
        | glossary["term-en"].str.contains(search_term, case=False, na=False)
    ]
    if not results.empty:
        st.write("### ğŸ“– Glossary Results")
        for _, row in results.iterrows():
            term_ko = row.get("term-ko", "")
            term_en = row.get("term-en", "")
            desc = row.get("description", "")
            st.markdown(f"**{term_ko}** / *{term_en}* â€” {desc}")
    else:
        st.info("No matching term found in glossary.")
