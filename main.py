import streamlit as st
import pandas as pd
import re
import os
import chardet

# --- íŒŒì¼ ê²½ë¡œ ì„¤ì • ---
KO_PATH = os.path.join("data", "urantia_ko.txt")
EN_PATH = os.path.join("data", "urantia_en.txt")
GLOSSARY_PATH = os.path.join("data", "glossary.xlsx")

# --- ìë™ ì¸ì½”ë”© ê°ì§€ ë° í…ìŠ¤íŠ¸ íŒŒì‹± ---
def parse_file(path):
    data = {}
    with open(path, "rb") as f:
        raw = f.read()
        detected = chardet.detect(raw)
        encoding = detected.get("encoding", "utf-8")
        text = raw.decode(encoding, errors="ignore")
        for line in text.splitlines():
            line = line.strip()
            match = re.match(r"(\d+:\d+\.\d+)\s+(.*)", line)
            if match:
                key = match.group(1).strip()
                data[key] = match.group(2).strip()
    return data

# --- ë°ì´í„° ë¡œë“œ (ìºì‹œ ì ìš©) ---
@st.cache_data
def load_texts():
    return parse_file(KO_PATH), parse_file(EN_PATH)

@st.cache_data
def load_glossary():
    df = pd.read_excel(GLOSSARY_PATH)
    df.columns = df.columns.str.lower()
    return df

# --- ì‹¤ì œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ---
try:
    ko_texts, en_texts = load_texts()
    glossary = load_glossary()
except Exception as e:
    st.error(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
    st.stop()

# --- Streamlit UI ---
st.title("ğŸ“˜ Urantia Book Viewer")
st.caption("Korean-English parallel viewer with glossary reference")

input_ref = st.text_input("ğŸ” Enter reference (e.g. 111:7.5):", "")

# --- ê²€ìƒ‰ ê¸°ëŠ¥ ---
if input_ref:
    ref = input_ref.strip()
    if ref in ko_texts:
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ‡°ğŸ‡· Korean Translation")
            st.write(f"**{ref}**  {ko_texts[ref]}")
        with col2:
            st.subheader("ğŸ‡ºğŸ‡¸ English Original")
            st.write(f"**{ref}**  {en_texts.get(ref, 'âŒ No English text found for this reference.')}")
    else:
        st.warning("No matching text found. Try nearby references or check your input.")

# --- ìš©ì–´ì§‘ ê²€ìƒ‰ ---
st.markdown("---")
search_term = st.text_input("ğŸ“– Search glossary term (Korean or English):", "")

if search_term:
    search_term = search_term.strip()
    results = glossary[
        glossary["term-ko"].str.contains(search_term, case=False, na=False)
        | glossary["termmen"].str.contains(search_term, case=False, na=False)
    ]
    if not results.empty:
        st.subheader("Glossary Results")
        for _, row in results.iterrows():
            term_ko = row.get("term-ko", "")
            term_en = row.get("termmen", "")
            desc = row.get("description", "")
            st.markdown(f"**{term_ko}** / *{term_en}* â€” {desc}")
    else:
        st.info("No matching term found in glossary.")
